{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/mnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fceb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /mnt/MyDrive/workspace/\n",
    "%cd \"/mnt/MyDrive/workspace/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/changmg/transaxx.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c8dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/mnt/MyDrive/workspace/transaxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca70277",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f0af6",
   "metadata": {},
   "source": [
    "# Model evaluation and re-training with TransAxx on CIFAR10 dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models.\n",
    "You can also retrain the model for further accuracy improvement\n",
    "\n",
    "**Note**:\n",
    "* Currently, the quantization bitwidth supported is 8bit and supported layers are Conv2d and Linear\n",
    "\n",
    "* Please make sure you have run the installation steps first\n",
    "\n",
    "* This example notebook approximates Conv2d layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31a01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.utils import *\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbe30f",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Set your path for the CIFAR10 dataset\n",
    "\n",
    "'calib dataset' is created from a 10% sample of train data for calibration purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd00e88a-3fab-48de-acc5-3a4a7fc7c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar10_data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:04<00:00, 34474389.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/cifar10_data/cifar-10-python.tar.gz to ./datasets/cifar10_data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "val_data, calib_data = cifar10_data_loader(data_path=\"./datasets/cifar10_data\", batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae3e54",
   "metadata": {},
   "source": [
    "## Select a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea9319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/anaconda3/envs/taxx-test/lib/python3.12/site-packages/torch/hub.py:293: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /home/chang/.cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/releases/download/repvgg/cifar10_repvgg_a0-ef08a50e.pt\" to /home/chang/.cache/torch/hub/checkpoints/cifar10_repvgg_a0-ef08a50e.pt\n",
      "100%|██████████| 30.1M/30.1M [00:00<00:00, 46.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# an example repo with cifar10 models. you can use your own (ref: https://github.com/chenyaofo/pytorch-cifar-models)\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", 'cifar10_repvgg_a0', pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2e8b9-0c1c-4a5e-b0f9-8c1c55d8dead",
   "metadata": {},
   "source": [
    "## Optional: Evaluate default model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212cd8c3-cdd3-47af-b7dc-ea677f9df40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:02<00:00, 28.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.856533169999693\n",
      "Accuracy of the network on the 10000 test images: 94.3209 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top1 = evaluate_cifar10(model, val_data, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4183a",
   "metadata": {},
   "source": [
    "## Initialize model with axx layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1064ebb6-2bdf-4523-981a-cab5dc3ae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get conv2d layers to approximate\n",
    "conv2d_layers = [(name, module) for name, module in model.named_modules() if (isinstance(module, torch.nn.Conv2d) or isinstance(module, AdaptConv2D)) and (\"head\" not in name and \"reduction\" not in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2db2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv2d_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b65a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/anaconda3/envs/taxx-test/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/home/chang/anaconda3/envs/taxx-test/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compile cuda extensions:  96.75404071807861\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with all required approximate multipliers for axx layers. \n",
    "# No explicit assignment needed; this step JIT compiles all upcoming multipliers\n",
    "\n",
    "axx_list = [{'axx_mult' : 'mul8s_acc', 'axx_power' : 1.0, 'quant_bits' : 8, 'fake_quant' : False}]*len(conv2d_layers)\n",
    "axx_list[3:4] = [{'axx_mult' : 'mul8s_1L2H', 'axx_power' : 0.7082, 'quant_bits' : 8, 'fake_quant' : False}] * 1\n",
    "\n",
    "start = time.time()\n",
    "replace_conv_layers(model,  AdaptConv2D, axx_list, 0, 0, layer_count=[0], returned_power = [0], initialize = True)  \n",
    "print('Time to compile cuda extensions: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "512ada14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module TensorQuantizer is treated as a zero-op.\n",
      "Warning: module RepVGGBlock is treated as a zero-op.\n",
      "Warning: module RepVGG is treated as a zero-op.\n",
      "Computational complexity:  491.95 MMacs\n",
      "Number of parameters::  7.84 MParams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/Work/WIP/transaxx/layers/adapt_convolution_layer.py:69: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# measure flops of model and compute 'flops' in every layer\n",
    "\n",
    "import io\n",
    "from classification.ptflops import get_model_complexity_info\n",
    "from classification.ptflops.pytorch_ops import linear_flops_counter_hook\n",
    "from classification.ptflops.pytorch_ops import conv_flops_counter_hook\n",
    "\n",
    "#hook our custom axx_layers in the appropriate flop counters, i.e. AdaptConv2D : conv_flops_counter_hook\n",
    "with torch.cuda.device(0):\n",
    "    total_macs, total_params, layer_specs = get_model_complexity_info(model, (3, 32, 32),as_strings=False, print_per_layer_stat=True,\n",
    "                                                          custom_modules_hooks={AdaptConv2D : conv_flops_counter_hook}, \n",
    "                                                          param_units='M', flops_units='MMac',\n",
    "                                                          verbose=True)\n",
    "\n",
    "print(f'Computational complexity:  {total_macs/1000000:.2f} MMacs')\n",
    "print(f'Number of parameters::  {total_params/1000000:.2f} MParams')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4f461",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the initial model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e67ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.53s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0202 12:35:37.755941 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.756675 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.757151 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.757489 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.758001 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.758291 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.758716 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.759108 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.759490 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.759748 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.760031 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.760243 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.760472 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.760701 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.760962 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.761230 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.761469 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.761728 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.761918 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.762169 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.762446 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.762726 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.764231 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.764442 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.765017 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.765365 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.765716 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.766067 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.766706 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.767078 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.767617 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.768057 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.768415 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.768887 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.769141 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.769403 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.769659 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.769910 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.770230 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.770746 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.771285 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.771681 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.771946 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.772183 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.772604 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.772885 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.773803 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.774179 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.774429 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.774691 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.774921 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.775154 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.775403 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.775608 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.775828 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.776150 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.776576 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.776842 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.777110 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.777642 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.777962 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.778315 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.778573 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.778785 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.779094 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.779398 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.779980 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.780363 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.780720 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.781129 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.781448 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.781698 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.781974 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.782229 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.782501 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.782818 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.783148 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.783468 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.784010 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.784290 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.784619 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.784883 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.785203 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.785454 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.786780 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.787044 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.787445 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.787776 135613386363520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 12:35:37.788424 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.788718 135613386363520 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0202 12:35:37.791135 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.791781 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.792296 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.792820 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.793785 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.794725 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.795244 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.795722 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.796495 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.797141 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.797807 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.798567 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.799269 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.799912 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.800869 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.801460 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.802150 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.803121 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.803953 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.804563 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.805301 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.806075 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.806710 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.807422 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.807938 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.808552 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.809023 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.809625 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.810198 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.810603 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.811063 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.811672 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.812439 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.812877 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.813385 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.813867 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.814348 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.815004 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.815737 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.816247 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.816760 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.817535 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.818102 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.818603 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.819140 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.819634 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.820076 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.820547 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.820984 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.821498 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.822018 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.822618 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.823408 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.824294 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.824809 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.825281 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.825690 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.826147 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.826598 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.827123 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.827685 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.828593 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.829143 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.829700 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.830372 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.830890 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.831483 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.832315 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.833056 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.833548 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.834027 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.834776 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.835374 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.835859 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.836319 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.836861 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.837292 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.837743 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.838370 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.839187 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.839977 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.840481 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.840939 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.841565 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.842014 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.842584 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 12:35:37.843019 135613386363520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage0.rbr_dense.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "stage0.rbr_dense.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.3628 calibrator=HistogramCalibrator scale=349.8851318359375 quant)\n",
      "stage0.rbr_1x1.conv.quantizer           : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "stage0.rbr_1x1.conv.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.4948 calibrator=HistogramCalibrator scale=256.5318603515625 quant)\n",
      "stage1.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.1008 calibrator=HistogramCalibrator scale=419.8192138671875 quant)\n",
      "stage1.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1891 calibrator=HistogramCalibrator scale=623.5847778320312 quant)\n",
      "stage1.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.1008 calibrator=HistogramCalibrator scale=419.8192138671875 quant)\n",
      "stage1.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=508.22552490234375 quant)\n",
      "stage1.1.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.5491 calibrator=HistogramCalibrator scale=0.001299710595048964 quant)\n",
      "stage1.1.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1906 calibrator=HistogramCalibrator scale=547.1300659179688 quant)\n",
      "stage1.1.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.5491 calibrator=HistogramCalibrator scale=0.001299710595048964 quant)\n",
      "stage1.1.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.2335 calibrator=HistogramCalibrator scale=543.671875 quant)\n",
      "stage2.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.7485 calibrator=HistogramCalibrator scale=0.0009104527998715639 quant)\n",
      "stage2.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1353 calibrator=HistogramCalibrator scale=793.5965576171875 quant)\n",
      "stage2.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.7485 calibrator=HistogramCalibrator scale=0.0009104527998715639 quant)\n",
      "stage2.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.1574 calibrator=HistogramCalibrator scale=806.3528442382812 quant)\n",
      "stage2.1.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=0.7757 calibrator=HistogramCalibrator scale=0.0019296294776722789 quant)\n",
      "stage2.1.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0746 calibrator=HistogramCalibrator scale=1353.4476318359375 quant)\n",
      "stage2.1.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=0.7757 calibrator=HistogramCalibrator scale=0.0019296294776722789 quant)\n",
      "stage2.1.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0735 calibrator=HistogramCalibrator scale=1728.2066650390625 quant)\n",
      "stage2.2.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2121 calibrator=HistogramCalibrator scale=0.00111574016045779 quant)\n",
      "stage2.2.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0845 calibrator=HistogramCalibrator scale=1104.7998046875 quant)\n",
      "stage2.2.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2121 calibrator=HistogramCalibrator scale=0.00111574016045779 quant)\n",
      "stage2.2.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0931 calibrator=HistogramCalibrator scale=1364.03125 quant)\n",
      "stage2.3.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.4875 calibrator=HistogramCalibrator scale=0.0005209675873629749 quant)\n",
      "stage2.3.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0994 calibrator=HistogramCalibrator scale=903.7467041015625 quant)\n",
      "stage2.3.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.4875 calibrator=HistogramCalibrator scale=0.0005209675873629749 quant)\n",
      "stage2.3.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.1354 calibrator=HistogramCalibrator scale=937.5003051757812 quant)\n",
      "stage3.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.6484 calibrator=HistogramCalibrator scale=0.0003970199031755328 quant)\n",
      "stage3.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1005 calibrator=HistogramCalibrator scale=740.3419799804688 quant)\n",
      "stage3.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.6484 calibrator=HistogramCalibrator scale=0.0003970199031755328 quant)\n",
      "stage3.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1036.6353759765625 quant)\n",
      "stage3.1.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=0.6896 calibrator=HistogramCalibrator scale=0.000951406720560044 quant)\n",
      "stage3.1.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0481 calibrator=HistogramCalibrator scale=1755.15380859375 quant)\n",
      "stage3.1.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6896 calibrator=HistogramCalibrator scale=0.000951406720560044 quant)\n",
      "stage3.1.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0500 calibrator=HistogramCalibrator scale=2334.70654296875 quant)\n",
      "stage3.2.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=0.9848 calibrator=HistogramCalibrator scale=0.00045488475007005036 quant)\n",
      "stage3.2.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0503 calibrator=HistogramCalibrator scale=1733.7703857421875 quant)\n",
      "stage3.2.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=0.9848 calibrator=HistogramCalibrator scale=0.00045488475007005036 quant)\n",
      "stage3.2.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0478 calibrator=HistogramCalibrator scale=2062.927734375 quant)\n",
      "stage3.3.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2430 calibrator=HistogramCalibrator scale=0.0007393453852273524 quant)\n",
      "stage3.3.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0590 calibrator=HistogramCalibrator scale=1473.2110595703125 quant)\n",
      "stage3.3.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2430 calibrator=HistogramCalibrator scale=0.0007393453852273524 quant)\n",
      "stage3.3.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0419 calibrator=HistogramCalibrator scale=2603.29345703125 quant)\n",
      "stage3.4.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2945 calibrator=HistogramCalibrator scale=0.0004984228871762753 quant)\n",
      "stage3.4.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0610 calibrator=HistogramCalibrator scale=1127.9583740234375 quant)\n",
      "stage3.4.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2945 calibrator=HistogramCalibrator scale=0.0004984228871762753 quant)\n",
      "stage3.4.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0491 calibrator=HistogramCalibrator scale=2113.308349609375 quant)\n",
      "stage3.5.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.3738 calibrator=HistogramCalibrator scale=0.0005940690753050148 quant)\n",
      "stage3.5.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0622 calibrator=HistogramCalibrator scale=1322.8944091796875 quant)\n",
      "stage3.5.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.3738 calibrator=HistogramCalibrator scale=0.0005940690753050148 quant)\n",
      "stage3.5.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0474 calibrator=HistogramCalibrator scale=2299.19873046875 quant)\n",
      "stage3.6.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.3062 calibrator=HistogramCalibrator scale=0.0007017696625553071 quant)\n",
      "stage3.6.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0542 calibrator=HistogramCalibrator scale=1451.494140625 quant)\n",
      "stage3.6.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.3062 calibrator=HistogramCalibrator scale=0.0007017696625553071 quant)\n",
      "stage3.6.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0357 calibrator=HistogramCalibrator scale=3030.50634765625 quant)\n",
      "stage3.7.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.3802 calibrator=HistogramCalibrator scale=0.00033015524968504906 quant)\n",
      "stage3.7.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0540 calibrator=HistogramCalibrator scale=1474.31591796875 quant)\n",
      "stage3.7.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.3802 calibrator=HistogramCalibrator scale=0.00033015524968504906 quant)\n",
      "stage3.7.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0291 calibrator=HistogramCalibrator scale=3242.998046875 quant)\n",
      "stage3.8.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.4555 calibrator=HistogramCalibrator scale=0.00046310710604302585 quant)\n",
      "stage3.8.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0540 calibrator=HistogramCalibrator scale=1615.5277099609375 quant)\n",
      "stage3.8.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.4555 calibrator=HistogramCalibrator scale=0.00046310710604302585 quant)\n",
      "stage3.8.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator scale=3954.505615234375 quant)\n",
      "stage3.9.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.5522 calibrator=HistogramCalibrator scale=0.0003686116251628846 quant)\n",
      "stage3.9.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0534 calibrator=HistogramCalibrator scale=1125.8734130859375 quant)\n",
      "stage3.9.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.5522 calibrator=HistogramCalibrator scale=0.0003686116251628846 quant)\n",
      "stage3.9.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator scale=1659.3878173828125 quant)\n",
      "stage3.10.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.5608 calibrator=HistogramCalibrator scale=0.0008287421660497785 quant)\n",
      "stage3.10.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0418 calibrator=HistogramCalibrator scale=1839.17333984375 quant)\n",
      "stage3.10.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.5608 calibrator=HistogramCalibrator scale=0.0008287421660497785 quant)\n",
      "stage3.10.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0228 calibrator=HistogramCalibrator scale=4612.30712890625 quant)\n",
      "stage3.11.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.5380 calibrator=HistogramCalibrator scale=0.00047514057951048017 quant)\n",
      "stage3.11.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=1487.13720703125 quant)\n",
      "stage3.11.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.5380 calibrator=HistogramCalibrator scale=0.00047514057951048017 quant)\n",
      "stage3.11.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=4610.83935546875 quant)\n",
      "stage3.12.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.4459 calibrator=HistogramCalibrator scale=0.0006209207931533456 quant)\n",
      "stage3.12.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0377 calibrator=HistogramCalibrator scale=2066.015869140625 quant)\n",
      "stage3.12.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.4459 calibrator=HistogramCalibrator scale=0.0006209207931533456 quant)\n",
      "stage3.12.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0213 calibrator=HistogramCalibrator scale=3852.668701171875 quant)\n",
      "stage3.13.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.1399 calibrator=HistogramCalibrator scale=0.004640910774469376 quant)\n",
      "stage3.13.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0353 calibrator=HistogramCalibrator scale=2128.9443359375 quant)\n",
      "stage3.13.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.1399 calibrator=HistogramCalibrator scale=0.004640910774469376 quant)\n",
      "stage3.13.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator scale=5691.36474609375 quant)\n",
      "stage4.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2038 calibrator=HistogramCalibrator scale=0.0025944889057427645 quant)\n",
      "stage4.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0085 calibrator=HistogramCalibrator scale=9720.8115234375 quant)\n",
      "stage4.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2038 calibrator=HistogramCalibrator scale=0.0025944889057427645 quant)\n",
      "stage4.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0174 calibrator=HistogramCalibrator scale=5896.40283203125 quant)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, calib_data, num_batches=2, device=device)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99, device=device)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a48a6e",
   "metadata": {},
   "source": [
    "## Run model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c927c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power of approximated operations:  94.43 %\n",
      "Model compiled. Running evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:29<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.628271305\n",
      "Accuracy of the network on the 10000 test images: 91.7167 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set desired approximate multiplier in each layer\n",
    "\n",
    "#at first, set all layers to have the 8-bit accurate multiplier\n",
    "axx_list = [{'axx_mult' : 'mul8s_acc', 'axx_power' : 1.0, 'quant_bits' : 8, 'fake_quant' : False}]*len(conv2d_layers)\n",
    "\n",
    "# For example, set the first 10 layers to be approximated with a specific multiplier \n",
    "axx_list[0:10] = [{'axx_mult' : 'mul8s_1L2H', 'axx_power' : 0.7082, 'quant_bits' : 8, 'fake_quant' : False}] * 10\n",
    "\n",
    "returned_power = [0]\n",
    "replace_conv_layers(model,  AdaptConv2D, axx_list, total_macs, total_params, layer_count=[0], returned_power = returned_power, initialize = False)  \n",
    "print('Power of approximated operations: ', round(returned_power[0], 2), '%')\n",
    "print('Model compiled. Running evaluation')\n",
    "\n",
    "# Run evaluation on the validation dataset\n",
    "top1 = evaluate_cifar10(model, val_data, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139533e-bab5-4144-9bf3-ad2497f7d839",
   "metadata": {},
   "source": [
    "## Run model retraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "182ef59c-c29c-4bf6-afc4-1b4fb1e6bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/39]  eta: 0:00:32  lr: 0.0001  img/s: 179.65246524378043  loss: 0.0849 (0.0849)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.8401  data: 0.1276  max mem: 756\n",
      "Epoch: [0]  [10/39]  eta: 0:00:13  lr: 0.0001  img/s: 306.99301065584785  loss: 0.0969 (0.0937)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (99.8580)  time: 0.4616  data: 0.0118  max mem: 779\n",
      "Epoch: [0]  [20/39]  eta: 0:00:08  lr: 0.0001  img/s: 297.4863353338605  loss: 0.0829 (0.0863)  acc1: 96.8750 (97.1726)  acc5: 100.0000 (99.8140)  time: 0.4226  data: 0.0002  max mem: 779\n",
      "Epoch: [0]  [30/39]  eta: 0:00:03  lr: 0.0001  img/s: 302.8545756703608  loss: 0.0817 (0.0869)  acc1: 97.6562 (97.2530)  acc5: 100.0000 (99.7732)  time: 0.4228  data: 0.0001  max mem: 779\n",
      "Epoch: [0] Total time: 0:00:16\n"
     ]
    }
   ],
   "source": [
    "from classification.train import train_one_epoch\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001) # set desired learning rate\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "#one epoch retrain\n",
    "train_one_epoch(model, criterion, optimizer, calib_data, device, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65da4f4-d0a9-4fe5-a70d-935ccb238c4a",
   "metadata": {},
   "source": [
    "## Re-run model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f6e1dd-07bb-4795-b0d6-34d7680f11c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:29<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.45351870100012\n",
      "Accuracy of the network on the 10000 test images: 91.9271 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top1 = evaluate_cifar10(model, val_data, device = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxx-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
