{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/mnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fceb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /mnt/MyDrive/workspace/\n",
    "%cd \"/mnt/MyDrive/workspace/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/changmg/transaxx.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c8dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/mnt/MyDrive/workspace/transaxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca70277",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f0af6",
   "metadata": {},
   "source": [
    "# Model evaluation and re-training with TransAxx on CIFAR10 dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models.\n",
    "You can also retrain the model for further accuracy improvement\n",
    "\n",
    "**Note**:\n",
    "* Currently, the quantization bitwidth supported is 8bit and supported layers are Conv2d and Linear\n",
    "\n",
    "* Please make sure you have run the installation steps first\n",
    "\n",
    "* This example notebook approximates Conv2d layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31a01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.utils import *\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbe30f",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Set your path for the CIFAR10 dataset\n",
    "\n",
    "'calib dataset' is created from a 10% sample of train data for calibration purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd00e88a-3fab-48de-acc5-3a4a7fc7c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "val_data, calib_data = cifar10_data_loader(data_path=\"./datasets/cifar10_data\", batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae3e54",
   "metadata": {},
   "source": [
    "## Select a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea9319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chang/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "# an example repo with cifar10 models. you can use your own (ref: https://github.com/chenyaofo/pytorch-cifar-models)\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", 'cifar10_repvgg_a0', pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2e8b9-0c1c-4a5e-b0f9-8c1c55d8dead",
   "metadata": {},
   "source": [
    "## Optional: Evaluate default model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212cd8c3-cdd3-47af-b7dc-ea677f9df40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:02<00:00, 33.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.481016346000615\n",
      "Accuracy of the network on the 10000 test images: 94.3209 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top1 = evaluate_cifar10(model, val_data, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4183a",
   "metadata": {},
   "source": [
    "## Initialize model with axx layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1064ebb6-2bdf-4523-981a-cab5dc3ae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get conv2d layers to approximate\n",
    "conv2d_layers = [(name, module) for name, module in model.named_modules() if (isinstance(module, torch.nn.Conv2d) or isinstance(module, AdaptConv2D)) and (\"head\" not in name and \"reduction\" not in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2db2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv2d_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b65a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Compute Architecture: sm_89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/anaconda3/envs/taxx-test/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/anaconda3/envs/taxx-test/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "Time to compile cuda extensions:  72.0421826839447\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with all required approximate multipliers for axx layers. \n",
    "# No explicit assignment needed; this step JIT compiles all upcoming multipliers\n",
    "\n",
    "axx_list = [{'axx_mult' : 'mul8s_acc', 'axx_power' : 1.0, 'quant_bits' : 8, 'fake_quant' : False}]*len(conv2d_layers)\n",
    "axx_list[3:4] = [{'axx_mult' : 'mul8s_1L2H', 'axx_power' : 0.7082, 'quant_bits' : 8, 'fake_quant' : False}] * 1\n",
    "\n",
    "start = time.time()\n",
    "replace_conv_layers(model,  AdaptConv2D, axx_list, 0, 0, layer_count=[0], returned_power = [0], initialize = True)  \n",
    "print('Time to compile cuda extensions: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512ada14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module TensorQuantizer is treated as a zero-op.\n",
      "Warning: module RepVGGBlock is treated as a zero-op.\n",
      "Warning: module RepVGG is treated as a zero-op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/Work/WIP/transaxx/layers/adapt_convolution_layer.py:75: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(quant_input, quant_weight, bias, stride, padding, dilation, groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:  491.95 MMacs\n",
      "Number of parameters::  7.84 MParams\n"
     ]
    }
   ],
   "source": [
    "# measure flops of model and compute 'flops' in every layer\n",
    "\n",
    "import io\n",
    "from classification.ptflops import get_model_complexity_info\n",
    "from classification.ptflops.pytorch_ops import linear_flops_counter_hook\n",
    "from classification.ptflops.pytorch_ops import conv_flops_counter_hook\n",
    "\n",
    "#hook our custom axx_layers in the appropriate flop counters, i.e. AdaptConv2D : conv_flops_counter_hook\n",
    "with torch.cuda.device(0):\n",
    "    total_macs, total_params, layer_specs = get_model_complexity_info(model, (3, 32, 32),as_strings=False, print_per_layer_stat=True,\n",
    "                                                          custom_modules_hooks={AdaptConv2D : conv_flops_counter_hook}, \n",
    "                                                          param_units='M', flops_units='MMac',\n",
    "                                                          verbose=True)\n",
    "\n",
    "print(f'Computational complexity:  {total_macs/1000000:.2f} MMacs')\n",
    "print(f'Number of parameters::  {total_params/1000000:.2f} MParams')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4f461",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the initial model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e67ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0202 13:15:54.835047 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.835716 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.836232 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.836864 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.837194 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.837452 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.837690 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.837910 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.838751 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.838996 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.839264 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.839488 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.839745 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.840327 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.840658 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.840934 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.841189 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.841708 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.841966 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.842174 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.842383 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.842673 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.843189 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.843523 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.844264 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.844482 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.844687 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.845211 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.845431 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.845674 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.845971 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.846280 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.846598 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.847039 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.847430 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.847778 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.848075 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.848350 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.848553 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.848810 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.849089 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.849357 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.849623 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.849980 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.850480 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.851030 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.851385 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.851637 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.851920 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.852163 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.852400 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.852653 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.852917 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.854680 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.854957 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.855178 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.855394 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.855610 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.855911 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.856365 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.856670 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.856974 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.857234 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.857517 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.857751 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.857972 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.858171 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.858669 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.858962 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.859325 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.859702 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.859996 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.861476 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.862032 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.862331 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.862987 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.863446 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.863770 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.864422 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.864688 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.864978 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.865204 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.865482 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.865820 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.866100 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.866503 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.867087 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.867513 135905308418688 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0202 13:15:54.868688 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.868938 135905308418688 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0202 13:15:54.870034 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.870934 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.871634 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.872313 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.872798 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.873425 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.873951 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.874431 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.875591 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.876219 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.876691 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.877240 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.877696 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.878157 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.878818 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.879441 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.880207 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.880659 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.881254 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.881813 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.882300 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.882903 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.883380 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.883906 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.884326 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.884804 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.885313 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.885764 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.886327 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.886816 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.887570 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.888431 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.888857 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.889467 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.890141 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.890884 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.891465 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.892041 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.892488 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.893070 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.893553 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.894251 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.894719 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.895546 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.896441 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.897327 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.897833 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.898387 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.898859 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.899350 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.899731 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.900268 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.900761 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.901337 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.901771 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.902222 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.902693 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.903225 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.903748 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.904453 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.904882 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.905346 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.905779 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.906483 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.906956 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.907399 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.907944 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.908489 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.908893 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.909421 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.910345 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.910899 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.911481 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.912031 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.912565 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.913206 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.913861 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.914422 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.914961 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.915440 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.916211 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.916929 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.917717 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.918295 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.918705 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.919203 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 13:15:54.919863 135905308418688 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage0.rbr_dense.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "stage0.rbr_dense.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.3628 calibrator=HistogramCalibrator scale=349.8851318359375 quant)\n",
      "stage0.rbr_1x1.conv.quantizer           : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "stage0.rbr_1x1.conv.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.4948 calibrator=HistogramCalibrator scale=256.5318603515625 quant)\n",
      "stage1.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.1008 calibrator=HistogramCalibrator scale=419.8192138671875 quant)\n",
      "stage1.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1891 calibrator=HistogramCalibrator scale=623.5847778320312 quant)\n",
      "stage1.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.1008 calibrator=HistogramCalibrator scale=419.8192138671875 quant)\n",
      "stage1.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=508.22552490234375 quant)\n",
      "stage1.1.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.5491 calibrator=HistogramCalibrator scale=0.001299710595048964 quant)\n",
      "stage1.1.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1906 calibrator=HistogramCalibrator scale=547.1300659179688 quant)\n",
      "stage1.1.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.5491 calibrator=HistogramCalibrator scale=0.001299710595048964 quant)\n",
      "stage1.1.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.2335 calibrator=HistogramCalibrator scale=543.671875 quant)\n",
      "stage2.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.7485 calibrator=HistogramCalibrator scale=0.0009104527998715639 quant)\n",
      "stage2.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1353 calibrator=HistogramCalibrator scale=793.5965576171875 quant)\n",
      "stage2.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.7485 calibrator=HistogramCalibrator scale=0.0009104527998715639 quant)\n",
      "stage2.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.1574 calibrator=HistogramCalibrator scale=806.3528442382812 quant)\n",
      "stage2.1.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=0.7757 calibrator=HistogramCalibrator scale=0.0019296294776722789 quant)\n",
      "stage2.1.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0746 calibrator=HistogramCalibrator scale=1353.4476318359375 quant)\n",
      "stage2.1.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=0.7757 calibrator=HistogramCalibrator scale=0.0019296294776722789 quant)\n",
      "stage2.1.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0735 calibrator=HistogramCalibrator scale=1728.2066650390625 quant)\n",
      "stage2.2.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2121 calibrator=HistogramCalibrator scale=0.00111574016045779 quant)\n",
      "stage2.2.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0845 calibrator=HistogramCalibrator scale=1104.7998046875 quant)\n",
      "stage2.2.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2121 calibrator=HistogramCalibrator scale=0.00111574016045779 quant)\n",
      "stage2.2.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0931 calibrator=HistogramCalibrator scale=1364.03125 quant)\n",
      "stage2.3.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.4875 calibrator=HistogramCalibrator scale=0.0005209675873629749 quant)\n",
      "stage2.3.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0994 calibrator=HistogramCalibrator scale=903.7467041015625 quant)\n",
      "stage2.3.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.4875 calibrator=HistogramCalibrator scale=0.0005209675873629749 quant)\n",
      "stage2.3.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.1354 calibrator=HistogramCalibrator scale=937.5003051757812 quant)\n",
      "stage3.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.6484 calibrator=HistogramCalibrator scale=0.0003970199031755328 quant)\n",
      "stage3.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1005 calibrator=HistogramCalibrator scale=740.3419799804688 quant)\n",
      "stage3.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.6484 calibrator=HistogramCalibrator scale=0.0003970199031755328 quant)\n",
      "stage3.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1036.6353759765625 quant)\n",
      "stage3.1.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=0.6896 calibrator=HistogramCalibrator scale=0.000951406720560044 quant)\n",
      "stage3.1.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0481 calibrator=HistogramCalibrator scale=1755.15380859375 quant)\n",
      "stage3.1.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6896 calibrator=HistogramCalibrator scale=0.000951406720560044 quant)\n",
      "stage3.1.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0500 calibrator=HistogramCalibrator scale=2334.70654296875 quant)\n",
      "stage3.2.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=0.9848 calibrator=HistogramCalibrator scale=0.00045488475007005036 quant)\n",
      "stage3.2.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0503 calibrator=HistogramCalibrator scale=1733.7703857421875 quant)\n",
      "stage3.2.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=0.9848 calibrator=HistogramCalibrator scale=0.00045488475007005036 quant)\n",
      "stage3.2.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0478 calibrator=HistogramCalibrator scale=2062.927734375 quant)\n",
      "stage3.3.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2430 calibrator=HistogramCalibrator scale=0.0007393453852273524 quant)\n",
      "stage3.3.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0590 calibrator=HistogramCalibrator scale=1473.2110595703125 quant)\n",
      "stage3.3.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2430 calibrator=HistogramCalibrator scale=0.0007393453852273524 quant)\n",
      "stage3.3.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0419 calibrator=HistogramCalibrator scale=2603.29345703125 quant)\n",
      "stage3.4.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2945 calibrator=HistogramCalibrator scale=0.0004984228871762753 quant)\n",
      "stage3.4.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0610 calibrator=HistogramCalibrator scale=1127.9583740234375 quant)\n",
      "stage3.4.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2945 calibrator=HistogramCalibrator scale=0.0004984228871762753 quant)\n",
      "stage3.4.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0491 calibrator=HistogramCalibrator scale=2113.308349609375 quant)\n",
      "stage3.5.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.3738 calibrator=HistogramCalibrator scale=0.0005940690753050148 quant)\n",
      "stage3.5.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0622 calibrator=HistogramCalibrator scale=1322.8944091796875 quant)\n",
      "stage3.5.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.3738 calibrator=HistogramCalibrator scale=0.0005940690753050148 quant)\n",
      "stage3.5.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0474 calibrator=HistogramCalibrator scale=2299.19873046875 quant)\n",
      "stage3.6.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.3062 calibrator=HistogramCalibrator scale=0.0007017696625553071 quant)\n",
      "stage3.6.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0542 calibrator=HistogramCalibrator scale=1451.494140625 quant)\n",
      "stage3.6.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.3062 calibrator=HistogramCalibrator scale=0.0007017696625553071 quant)\n",
      "stage3.6.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0357 calibrator=HistogramCalibrator scale=3030.50634765625 quant)\n",
      "stage3.7.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.3802 calibrator=HistogramCalibrator scale=0.00033015524968504906 quant)\n",
      "stage3.7.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0540 calibrator=HistogramCalibrator scale=1474.31591796875 quant)\n",
      "stage3.7.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.3802 calibrator=HistogramCalibrator scale=0.00033015524968504906 quant)\n",
      "stage3.7.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0291 calibrator=HistogramCalibrator scale=3242.998046875 quant)\n",
      "stage3.8.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.4555 calibrator=HistogramCalibrator scale=0.00046310710604302585 quant)\n",
      "stage3.8.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0540 calibrator=HistogramCalibrator scale=1615.5277099609375 quant)\n",
      "stage3.8.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.4555 calibrator=HistogramCalibrator scale=0.00046310710604302585 quant)\n",
      "stage3.8.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator scale=3954.505615234375 quant)\n",
      "stage3.9.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.5522 calibrator=HistogramCalibrator scale=0.0003686116251628846 quant)\n",
      "stage3.9.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0534 calibrator=HistogramCalibrator scale=1125.8734130859375 quant)\n",
      "stage3.9.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.5522 calibrator=HistogramCalibrator scale=0.0003686116251628846 quant)\n",
      "stage3.9.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator scale=1659.3878173828125 quant)\n",
      "stage3.10.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.5608 calibrator=HistogramCalibrator scale=0.0008287421660497785 quant)\n",
      "stage3.10.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0418 calibrator=HistogramCalibrator scale=1839.17333984375 quant)\n",
      "stage3.10.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.5608 calibrator=HistogramCalibrator scale=0.0008287421660497785 quant)\n",
      "stage3.10.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0228 calibrator=HistogramCalibrator scale=4612.30712890625 quant)\n",
      "stage3.11.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.5380 calibrator=HistogramCalibrator scale=0.00047514057951048017 quant)\n",
      "stage3.11.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=1487.13720703125 quant)\n",
      "stage3.11.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.5380 calibrator=HistogramCalibrator scale=0.00047514057951048017 quant)\n",
      "stage3.11.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=4610.83935546875 quant)\n",
      "stage3.12.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.4459 calibrator=HistogramCalibrator scale=0.0006209207931533456 quant)\n",
      "stage3.12.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0377 calibrator=HistogramCalibrator scale=2066.015869140625 quant)\n",
      "stage3.12.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.4459 calibrator=HistogramCalibrator scale=0.0006209207931533456 quant)\n",
      "stage3.12.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0213 calibrator=HistogramCalibrator scale=3852.668701171875 quant)\n",
      "stage3.13.rbr_dense.conv.quantizer      : TensorQuantizer(8bit per-tensor amax=1.1399 calibrator=HistogramCalibrator scale=0.004640910774469376 quant)\n",
      "stage3.13.rbr_dense.conv.quantizer_w    : TensorQuantizer(8bit per-tensor amax=0.0353 calibrator=HistogramCalibrator scale=2128.9443359375 quant)\n",
      "stage3.13.rbr_1x1.conv.quantizer        : TensorQuantizer(8bit per-tensor amax=1.1399 calibrator=HistogramCalibrator scale=0.004640910774469376 quant)\n",
      "stage3.13.rbr_1x1.conv.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator scale=5691.36474609375 quant)\n",
      "stage4.0.rbr_dense.conv.quantizer       : TensorQuantizer(8bit per-tensor amax=1.2038 calibrator=HistogramCalibrator scale=0.0025944889057427645 quant)\n",
      "stage4.0.rbr_dense.conv.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.0085 calibrator=HistogramCalibrator scale=9720.8115234375 quant)\n",
      "stage4.0.rbr_1x1.conv.quantizer         : TensorQuantizer(8bit per-tensor amax=1.2038 calibrator=HistogramCalibrator scale=0.0025944889057427645 quant)\n",
      "stage4.0.rbr_1x1.conv.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0174 calibrator=HistogramCalibrator scale=5896.40283203125 quant)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, calib_data, num_batches=2, device=device)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99, device=device)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a48a6e",
   "metadata": {},
   "source": [
    "## Run model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c927c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "CUDA Compute Architecture: sm_89\n",
      "Power of approximated operations:  94.43 %\n",
      "Model compiled. Running evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:30<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.05098044100032\n",
      "Accuracy of the network on the 10000 test images: 91.7167 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set desired approximate multiplier in each layer\n",
    "\n",
    "#at first, set all layers to have the 8-bit accurate multiplier\n",
    "axx_list = [{'axx_mult' : 'mul8s_acc', 'axx_power' : 1.0, 'quant_bits' : 8, 'fake_quant' : False}]*len(conv2d_layers)\n",
    "\n",
    "# For example, set the first 10 layers to be approximated with a specific multiplier \n",
    "axx_list[0:10] = [{'axx_mult' : 'mul8s_1L2H', 'axx_power' : 0.7082, 'quant_bits' : 8, 'fake_quant' : False}] * 10\n",
    "\n",
    "returned_power = [0]\n",
    "replace_conv_layers(model,  AdaptConv2D, axx_list, total_macs, total_params, layer_count=[0], returned_power = returned_power, initialize = False)  \n",
    "print('Power of approximated operations: ', round(returned_power[0], 2), '%')\n",
    "print('Model compiled. Running evaluation')\n",
    "\n",
    "# Run evaluation on the validation dataset\n",
    "top1 = evaluate_cifar10(model, val_data, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139533e-bab5-4144-9bf3-ad2497f7d839",
   "metadata": {},
   "source": [
    "## Run model retraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "182ef59c-c29c-4bf6-afc4-1b4fb1e6bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/39]  eta: 0:00:32  lr: 0.0001  img/s: 179.65246524378043  loss: 0.0849 (0.0849)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.8401  data: 0.1276  max mem: 756\n",
      "Epoch: [0]  [10/39]  eta: 0:00:13  lr: 0.0001  img/s: 306.99301065584785  loss: 0.0969 (0.0937)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (99.8580)  time: 0.4616  data: 0.0118  max mem: 779\n",
      "Epoch: [0]  [20/39]  eta: 0:00:08  lr: 0.0001  img/s: 297.4863353338605  loss: 0.0829 (0.0863)  acc1: 96.8750 (97.1726)  acc5: 100.0000 (99.8140)  time: 0.4226  data: 0.0002  max mem: 779\n",
      "Epoch: [0]  [30/39]  eta: 0:00:03  lr: 0.0001  img/s: 302.8545756703608  loss: 0.0817 (0.0869)  acc1: 97.6562 (97.2530)  acc5: 100.0000 (99.7732)  time: 0.4228  data: 0.0001  max mem: 779\n",
      "Epoch: [0] Total time: 0:00:16\n"
     ]
    }
   ],
   "source": [
    "from classification.train import train_one_epoch\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001) # set desired learning rate\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "#one epoch retrain\n",
    "train_one_epoch(model, criterion, optimizer, calib_data, device, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65da4f4-d0a9-4fe5-a70d-935ccb238c4a",
   "metadata": {},
   "source": [
    "## Re-run model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f6e1dd-07bb-4795-b0d6-34d7680f11c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:29<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.45351870100012\n",
      "Accuracy of the network on the 10000 test images: 91.9271 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top1 = evaluate_cifar10(model, val_data, device = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxx-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
